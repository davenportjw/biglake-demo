# Requires execution to have BigQuery Admin Role
# Sample Request:
# {"connectionName":"test_connection_3","location":"US","projectId":"eventarc-tf-test-5"}

main:
    params: [args]
    steps:
    - assign_values:
        assign:
            - project_id: eventarc-tf-test-4 # ${args.projectId}
            - location: us-central1 # ${args.location}
            - connection_name: bq_spark_connection # ${args.connectionName}
            - batch_name: lakehouse-iceberg-02
            - dataproc_service_account: dataproc-sa-8a68acad@eventarc-tf-test-4.iam.gserviceaccount.com
            - provisioner_bucket_name: gcp-lakehouse-provisioner-8a68acad
            - warehouse_bucket_name: gcp-lakehouse-raw-8a68acad
            - temp_bucket_name: gcp-lakehouse-raw-8a68acad
            - lakehouse_catalog: lakehouse_catalog
            - lakehouse_database: lakehouse_database
            - bq_dataset: gcp_lakehouse
            - bq_gcs_connection: us-central1.gcp_gcs_connection
    - dataflow_serverless_job:
        call: http.post
        args:
            url: ${"https://dataproc.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/batches"}
            auth:
                type: OAuth2
            body:
                pysparkBatch: 
                    mainPythonFileUri: ${"gs://"+provisioner_bucket_name+"/bigquery.py"}
                    jarFileUris: 
                        - "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar"
                        - "gs://spark-lib/biglake/iceberg-biglake-catalog-0.0.1-with-dependencies.jar"
                runtimeConfig: 
                    version: "1.0.29"
                    properties: 
                        "spark.sql.catalog.lakehouse_catalog": "org.apache.iceberg.spark.SparkCatalog"
                        "spark.sql.catalog.lakehouse_catalog.blms_catalog": "${lakehouse_catalog}"
                        "spark.sql.catalog.lakehouse_catalog.catalog-impl": "org.apache.iceberg.gcp.biglake.BigLakeCatalog"
                        "spark.sql.catalog.lakehouse_catalog.gcp_location": "${location}"
                        "spark.sql.catalog.lakehouse_catalog.gcp_project": "${project_id}"
                        "spark.sql.catalog.lakehouse_catalog.warehouse": ${"gs://"+warehouse_bucket_name+"/warehouse"}
                        "spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.14.1"
                        lakehouse_catalog: ${lakehouse_catalog}
                        lakehouse_database: ${lakehouse_database}
                        temp_bucket: ${temp_bucket_name}
                        bq_dataset: ${bq_dataset}
                        bq_gcs_connection: ${bq_gcs_connection}
                
                environmentConfig: 
                    executionConfig:
                        serviceAccount: ${dataproc_service_account}
                        subnetworkUri: "dataproc-subnet"
            query:
                batchId: ${batch_name}
            timeout: 300
        result: Operation
    # TODO: add await result until Operation is complete


    - returnOutput:
            return: Operation