# Requires execution to have BigQuery Admin Role
# Sample Request:
# {"connectionName":"test_connection_3","location":"US","projectId":"eventarc-tf-test-5"}

main:
    params: [args]
    steps:
    - assign_values:
        assign:
            - project_id: eventarc-tf-test-4 # ${args.projectId}
            - location: us-central1 # ${args.location}
            - connection_name: bq_spark_connection # ${args.connectionName}
            - batch_name: lakehouse_iceberg_01
            - dataproc_service_account: dataproc-sa-8a68acad@eventarc-tf-test-4.iam.gserviceaccount.com
            - provisioner_bucket_name: gcp-lakehouse-provisioner-8a68acad
            - warehouse_bucket_name: gcp-lakehouse-raw-8a68acad
    # - bq_spark_connection:
    #     call: http.post
    #     args:
    #         url: ${"https://bigqueryconnection.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/connections"}
    #         auth:
    #             type: OAuth2
    #         headers:
    #             Content-Type: application/json
    #         body:
    #             spark: {}
    #         query:
    #             connectionId: ${connection_name}
    #         timeout: 120
    #     result: Connection
    - dataflow_serverless_job:
        call: http.post
        args:
            url: ${"https://dataproc.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/batches"}
            auth:
                type: OAuth2
            headers:
                Content-Type: application-json
            body:
                pysparkBatch: 
                    mainPythonFileUri": ${"gs://"+provisioner_bucket_name+"/bigquery.py"}
                    jarFileUris": 
                        - "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar"
                        - "gs://spark-lib/biglake/iceberg-biglake-catalog-0.0.1-with-dependencies.jar"
                runtimeConfig: 
                    version: "1.0.29"
                    properties: 
                        - "spark:spark.sql.catalog.lakehouse_catalog": "org.apache.iceberg.spark.SparkCatalog"
                        - "spark:spark.sql.catalog.lakehouse_catalog.blms_catalog": "lakehouse_catalog"
                        - "spark:spark.sql.catalog.lakehouse_catalog.catalog-impl": "org.apache.iceberg.gcp.biglake.BigLakeCatalog"
                        - "spark:spark.sql.catalog.lakehouse_catalog.gcp_location": ${location}
                        - "spark:spark.sql.catalog.lakehouse_catalog.gcp_project": ${project_id}
                        - "spark:spark.sql.catalog.lakehouse_catalog.warehouse": ${"gs://"+warehouse_bucket_name+"/warehouse"}
                        - "spark:spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.14.1"
                environmentConfig: 
                    executionConfig:
                        serviceAccount: ${dataproc_service_account}
                        subnetworkUri: "dataproc-subnet"
            query:
                batchId: batch_name
            timeout: 300
        result: Operation


    - returnOutput:
            return: ${Connection.body.spark.serviceAccountId}

custom_predicate:
  params: [e]
  steps:
    - what_to_repeat:
        switch:
          - condition: ${e.code == 409}
            return: true
    - otherwise:
        return: false
